{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "257c16cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hghaan/rerank_model\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m cohere_api=Cohere(setting)\n\u001b[32m     26\u001b[39m bert=Bert_Extract(setting)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m rerank_model_fintuned=\u001b[43mRerankModelFinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m sentences_transformer_embedding=Sentences_Transformer_Embedding(setting)\n\u001b[32m     29\u001b[39m qdrant=Qdrant_Vector(setting,sentences_transformer_embedding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\DaiHoc/machinelearning/TLCN/DoAnTotNghiep_chat_bot\\source\\model\\rerank_model_finetune.py:6\u001b[39m, in \u001b[36mRerankModelFinetune.__init__\u001b[39m\u001b[34m(self, setting)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,setting:Settings ):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mCrossEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRERANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hdang\\.virtualenvs\\TLCN-KU7o-pax\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\util.py:39\u001b[39m, in \u001b[36mcross_encoder_init_args_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mconfig_kwargs\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mclassifier_dropout\u001b[39m\u001b[33m\"\u001b[39m] = classifier_dropout\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hdang\\.virtualenvs\\TLCN-KU7o-pax\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:178\u001b[39m, in \u001b[36mCrossEncoder.__init__\u001b[39m\u001b[34m(self, model_name_or_path, num_labels, max_length, activation_fn, device, cache_folder, trust_remote_code, revision, local_files_only, token, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m     config.num_labels = num_labels\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mand\u001b[39;00m max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    191\u001b[39m     tokenizer_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hdang\\.virtualenvs\\TLCN-KU7o-pax\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:239\u001b[39m, in \u001b[36mCrossEncoder._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, backend, **model_kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_model\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    233\u001b[39m     model_name_or_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     **model_kwargs,\n\u001b[32m    237\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[38;5;28mself\u001b[39m.model: PreTrainedModel = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m backend == \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    245\u001b[39m         \u001b[38;5;28mself\u001b[39m._load_onnx_model(model_name_or_path, config, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hdang\\.virtualenvs\\TLCN-KU7o-pax\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    563\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    568\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hdang\\.virtualenvs\\TLCN-KU7o-pax\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hdang\\.virtualenvs\\TLCN-KU7o-pax\\Lib\\site-packages\\transformers\\modeling_utils.py:4450\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4442\u001b[39m is_from_file = pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4445\u001b[39m     is_safetensors_available()\n\u001b[32m   4446\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[32m   4447\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[32m   4448\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[32m0\u001b[39m].endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4449\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4450\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   4451\u001b[39m         metadata = f.metadata()\n\u001b[32m   4453\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4454\u001b[39m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "project_path = r\"D:/DaiHoc/machinelearning/TLCN/DoAnTotNghiep_chat_bot/\"\n",
    "sys.path.append(project_path)\n",
    "from source.function.utils_result import RAG\n",
    "from source.search.utils_search import Qdrant_Utils\n",
    "from source.rerank.utils_rerank import Rerank_Utils  \n",
    "from source.model.embedding_model import Sentences_Transformer_Embedding\n",
    "from source.model.extract_model import Bert_Extract\n",
    "from source.model.generate_model import Gemini\n",
    "from source.model.rerank_model import Cohere\n",
    "from source.data.vectordb.qdrant import Qdrant_Vector\n",
    "from source.core.config import Settings\n",
    "from source.model.rerank_model_finetune import RerankModelFinetune\n",
    "from source.generate.generate import Gemini_Generate\n",
    "from source.extract.utils_extract import Extract_Information\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "import cohere\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "setting=Settings()\n",
    "gemini=Gemini(setting)\n",
    "print(setting.RERANK)\n",
    "cohere_api=Cohere(setting)\n",
    "bert=Bert_Extract(setting)\n",
    "rerank_model_fintuned=RerankModelFinetune(setting)\n",
    "sentences_transformer_embedding=Sentences_Transformer_Embedding(setting)\n",
    "qdrant=Qdrant_Vector(setting,sentences_transformer_embedding)\n",
    "rerank_Utils=Rerank_Utils(cohere_api,rerank_model_fintuned)\n",
    "extract_Utils= Extract_Information(bert)\n",
    "generate_Utils=Gemini_Generate(gemini,setting)\n",
    "qdrant_Utils=Qdrant_Utils(qdrant, generate_Utils)\n",
    "rag=RAG(generate_Utils,extract_Utils,qdrant_Utils,rerank_Utils,setting,sentences_transformer_embedding)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hghaan/rerank_model\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "project_path = r\"D:/DaiHoc/machinelearning/TLCN/DoAnTotNghiep_chat_bot/\"\n",
    "sys.path.append(project_path)\n",
    "from source.function.utils_result import RAG\n",
    "from source.search.utils_search import Qdrant_Utils\n",
    "from source.rerank.utils_rerank import Rerank_Utils  \n",
    "from source.model.embedding_model import Sentences_Transformer_Embedding\n",
    "from source.model.extract_model import Bert_Extract\n",
    "from source.model.generate_model import Gemini\n",
    "from source.model.rerank_model import Cohere\n",
    "from source.data.vectordb.qdrant import Qdrant_Vector\n",
    "from source.core.config import Settings\n",
    "from source.model.rerank_model_finetune import RerankModelFinetune\n",
    "from source.generate.generate import Gemini_Generate\n",
    "from source.extract.utils_extract import Extract_Information\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "import cohere\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "setting=Settings()\n",
    "gemini=Gemini(setting)\n",
    "print(setting.RERANK)\n",
    "cohere_api=Cohere(setting)\n",
    "bert=Bert_Extract(setting)\n",
    "rerank_model_fintuned=RerankModelFinetune(setting)\n",
    "sentences_transformer_embedding=Sentences_Transformer_Embedding(setting)\n",
    "qdrant=Qdrant_Vector(setting,sentences_transformer_embedding)\n",
    "rerank_Utils=Rerank_Utils(cohere_api,rerank_model_fintuned)\n",
    "extract_Utils= Extract_Information(bert)\n",
    "generate_Utils=Gemini_Generate(gemini,setting)\n",
    "qdrant_Utils=Qdrant_Utils(qdrant, generate_Utils)\n",
    "rag=RAG(generate_Utils,extract_Utils,qdrant_Utils,rerank_Utils,setting,sentences_transformer_embedding)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280d5d7",
   "metadata": {},
   "source": [
    "## 1.Eval Gemini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7218d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc file CSV chứa câu hỏi và câu trả lời đã có (nếu có)\n",
    "df = pd.read_csv('./data/data_processed/final_data_system_response.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4031e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = r'Xin\\s+lỗi\\s+bạn'  # regex cho “Xin lỗi bạn” (có thể có nhiều khoảng trắng)\n",
    "\n",
    "# Chỉ giữ lại những dòng không khớp pattern\n",
    "df = df[~df['answer_from_gemini_rag_basic'].str.contains(pattern, regex=True, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b5a9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['answer_from_gemini_rag_final'].str.contains(r'\"\"', regex=True, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47916d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = r'Xin\\s+lỗi\\s+bạn'  # regex cho “Xin lỗi bạn” (có thể có nhiều khoảng trắng)\n",
    "\n",
    "# Chỉ giữ lại những dòng không khớp pattern\n",
    "df = df[~df['answer_from_gemini_rag_final_extract'].str.contains(pattern, regex=True, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23388ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b87605d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f6ccc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abb9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sinh trả lời:   0%|          | 0/564 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sinh trả lời:  37%|███▋      | 207/564 [1:05:14<5:52:34, 59.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: status_code: 502, body: \n",
      "<html><head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<title>502 Server Error</title>\n",
      "</head>\n",
      "<body text=#000000 bgcolor=#ffffff>\n",
      "<h1>Error: Server Error</h1>\n",
      "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
      "<h2></h2>\n",
      "</body></html>\n",
      "\n",
      "\n",
      "❌ Lỗi tại index 206 (DataFrame index = 206):\n",
      "Một số gia đình khi thuê người giúp việc gia đình vẫn thường than phiền khi tết đến phải cho người giúp việc tiền tàu xe, còn sợ sau tết họ lại bỏ việc. Đề nghị cho biết pháp luật quy định về vấn đề này như thế nào?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sinh trả lời: 100%|██████████| 564/564 [2:52:13<00:00, 18.32s/it]  \n"
     ]
    }
   ],
   "source": [
    "def Generate_Response(df_batch):\n",
    "    answers = []\n",
    "    for idx, question in tqdm(list(enumerate(df_batch['question'])), desc=\"Sinh trả lời\"):\n",
    "        try:\n",
    "            article_doc = rag.get_gemini_response_rag_final(question)\n",
    "            answers.append(article_doc)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Lỗi tại index {idx} (DataFrame index = {df_batch.index[idx]}):\\n{question}\")\n",
    "            answers.append(\"\")\n",
    "    return answers\n",
    "\n",
    "\n",
    "df['answer_from_gemini_rag_final'] = Generate_Response(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51edd72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Đã cập nhật và lưu toàn bộ vào: ./data/data_processed/final_data_system_response.csv\n"
     ]
    }
   ],
   "source": [
    "final_output_path = './data/data_processed/final_data_system_response.csv'\n",
    "df.to_csv(final_output_path, index=False)\n",
    "print(f\" Đã cập nhật và lưu toàn bộ vào: {final_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c857fd",
   "metadata": {},
   "source": [
    "> Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd43c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting=Settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99561e",
   "metadata": {},
   "source": [
    "> Get embedding gemini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d5d6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a4e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=gemini.key_manager.get_next_key())\n",
    "\n",
    "def get_gemini_embedding(texts):\n",
    "    try :\n",
    "        result = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=texts,\n",
    "            task_type=\"SEMANTIC_SIMILARITY\"\n",
    "        )\n",
    "        \n",
    "        return result['embedding']\n",
    "    except Exception as e:\n",
    "        print(\"Đã xảy ra lỗi:\", e)\n",
    "        return [0.0] * 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd6dbd",
   "metadata": {},
   "source": [
    "> get embedding cohere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06167d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "def get_cohere_embedding(text: str, model_name: str = \"embed-v4.0\") -> list:\n",
    "    try:\n",
    "        co = cohere.ClientV2(api_key=cohere_api.key_manager.get_next_key())\n",
    "        res = co.embed(\n",
    "            texts=[text],\n",
    "            model=model_name,\n",
    "            input_type=\"classification\",\n",
    "            embedding_types=[\"float\"],\n",
    "        )\n",
    "\n",
    "        # Trả về nhúng đầu tiên\n",
    "        return res.embeddings.float[0]\n",
    "    except Exception as e:\n",
    "        print(\"Đã xảy ra lỗi:\", e)\n",
    "        return [0.0] * 1024  # Trả về danh sách 1024 số 0 nếu có lỗi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6189d6",
   "metadata": {},
   "source": [
    ">Get  rouge-L & F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "251b6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_rouge_l(pred: str, ref: str) -> float:\n",
    "    try:\n",
    "        score = scorer.score(ref, pred)\n",
    "        return score['rougeL'].fmeasure\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def compute_f1(pred: str, ref: str) -> float:\n",
    "    pred_tokens = pred.lower().split()\n",
    "    ref_tokens = ref.lower().split()\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(ref_tokens)\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2863838d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa0009fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xảy ra lỗi: 400 Request payload size exceeds the limit: 36000 bytes.\n",
      "Đã xảy ra lỗi: 400 Request payload size exceeds the limit: 36000 bytes.\n"
     ]
    }
   ],
   "source": [
    "# df['answer_embedding_cohere'] = df['answer'].apply(get_cohere_embedding)\n",
    "# df['answer_from_gemini_rag_final_cohere'] = df['answer_from_gemini_rag_final'].apply(get_cohere_embedding)\n",
    "\n",
    "\n",
    "df['answer_embedding'] = df['answer'].apply(get_gemini_embedding)\n",
    "df['answer_from_gemini_rag_final_embedding'] = df['answer_from_gemini_rag_final'].apply(get_gemini_embedding)\n",
    "\n",
    "df['cosine_similarity_gemini_rag_final'] = df.apply(\n",
    "    lambda row: cosine_similarity(\n",
    "        [row['answer_embedding']], \n",
    "        [row['answer_from_gemini_rag_final_embedding']]\n",
    "    )[0][0],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# df['cosine_similarity_cohere'] = df.apply(\n",
    "#     lambda row: cosine_similarity(\n",
    "#         [row['answer_embedding_cohere']], \n",
    "#         [row['answer_from_gemini_rag_final_cohere']]\n",
    "#     )[0][0],\n",
    "#     axis=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6c5bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rouge_l'] = df.apply(lambda row: compute_rouge_l(row['answer_from_gemini_rag_final'], row['answer']), axis=1)\n",
    "df['f1'] = df.apply(lambda row: compute_f1(row['answer_from_gemini_rag_final'], row['answer']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b95c6f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trung bình Cosine (Gemini): 0.9038\n",
      " Trung bình ROUGE-L: 0.4171\n",
      " Trung bình F1 Score: 0.2266\n"
     ]
    }
   ],
   "source": [
    "print(f\" Trung bình Cosine (Gemini): {df['cosine_similarity_gemini_rag_final'].mean():.4f}\")\n",
    "print(f\" Trung bình ROUGE-L: {df['rouge_l'].mean():.4f}\")\n",
    "print(f\" Trung bình F1 Score: {df['f1'].mean():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50999835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLCN-KU7o-pax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
