{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hdang\\.virtualenvs\\machinelearning-lTTH8rYd\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "project_path = r\"D:/DaiHoc/machinelearning/TLCN/DoAnTotNghiep_chat_bot/\"\n",
    "sys.path.append(project_path)\n",
    "from source.function.utils_result import RAG\n",
    "from source.search.utils_search import Qdrant_Utils\n",
    "from source.rerank.utils_rerank import Rerank_Utils  \n",
    "from source.model.embedding_model import Sentences_Transformer_Embedding\n",
    "from source.model.extract_model import Bert_Extract\n",
    "from source.model.generate_model import Gemini\n",
    "from source.model.rerank_model import Cohere\n",
    "from source.data.vectordb.qdrant import Qdrant_Vector\n",
    "from source.core.config import Settings\n",
    "from source.function.utils_shared import extract_json_dict,load_prompt_from_yaml\n",
    "from source.generate.generate import Gemini_Generate\n",
    "from source.extract.utils_extract import Extract_Information\n",
    "from source.schema.chatbot_querry import ChatbotQuery\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchAny\n",
    "from source.tool.google_search import GoogleSearchTool\n",
    "setting=Settings()\n",
    "gemini=Gemini(setting)\n",
    "cohere=Cohere(setting)\n",
    "bert=Bert_Extract(setting)\n",
    "sentences_transformer_embedding=Sentences_Transformer_Embedding(setting)\n",
    "qdrant=Qdrant_Vector(setting,sentences_transformer_embedding)\n",
    "rerank_Utils=Rerank_Utils(cohere)\n",
    "extract_Utils= Extract_Information(bert)\n",
    "generate_Utils=Gemini_Generate(gemini,setting)\n",
    "qdrant_Utils=Qdrant_Utils(qdrant, generate_Utils)\n",
    "rag=RAG(generate_Utils,extract_Utils,qdrant_Utils,rerank_Utils,setting,sentences_transformer_embedding)\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import json\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=load_prompt_from_yaml(setting,\"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'original_query'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='### ROLE:\\nYou are a Vietnamese Legal Consultant with over 30 years of professional experience.\\n\\n### TASKS:\\nYou must answer all user questions related to Vietnamese law based solely on the reference content provided below, which has been retrieved from our vector database. You are strictly required to cite the specific reference(s) used to construct your answer.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'original_query'], input_types={}, partial_variables={}, template='### INPUT:\\n\\'{original_query}\\'\\n\\n### REFERENCE CONTENT:\\n\\'{context}\\'\\n\\n### GUIDELINES:\\n\\n#### 1. Question Understanding:\\n- Carefully interpret the question, including possible rewordings, synonyms, or semantic variations.\\n- Identify the core legal issue(s) that must be addressed.\\n\\n#### 2. Answering Rules:\\n- Answers must be strictly based on the provided reference documents.\\n- You are allowed to **infer information only when it is logically implied by the provided content.\\n- Absolutely do not fabricate or assume any information that is not explicitly or inferentially present in the documents — especially legal elements such as articles, clauses, or provisions.\\n- Responses must be in Vietnamese legal language, formal and professional.\\n\\n#### 3. Answer Structure:\\n- Begin with a brief introduction to the core issue.\\n- Use ordinal numbers or bullet points to clearly separate legal arguments.\\n- Each key legal point must be in its own paragraph.\\n- Ensure the explanation is **clear, thorough, and properly referenced**.\\n- Provide article numbers, clause references, and legal document titles where applicable.\\n- Conclude with a concise summary if appropriate.\\n\\n\\n#### 4. In Case of No Sufficient Information:\\nIf the context does not contain enough information to answer the question, return this message in Vietnamese:\\n> \"Xin lỗi bạn. Kiến thức này nằm ngoài phạm vi hiểu biết của tôi. Bạn có thể hỏi tôi một câu hỏi khác không? Tôi sẽ cố gắng giải đáp câu hỏi của bạn!\"\\n\\n#### 5. Answer Format:\\nYour final output must follow this JSON structure:\\n```json\\n{{\\n  \"answer\": \"Your detailed legal answer in Vietnamese, with properly formatted citations like (reference document 1), (reference document 2)...\",\\n  \"key\": [IDs of referenced documents used, e.g., 2, 3]\\n}}\\n```\\n\\n#### 6. Citation Format Rules:\\n- In the **answer** content, do not cite documents using their original IDs.\\n- Refer to each document by its **position** in the **key** list, starting from **1**.\\n- Citations must follow the format: **(reference document X)**, where **X is the index number of the document in the **key** list, **not the actual document ID**.\\n##### 6.1 Example: \\n- If **key**: [4, 7, 3] , then:\\n    + Document with ID 4 becomes reference document **1**.\\n    + Document with ID 7 becomes reference document **2**.\\n    + Document with ID 3 becomes reference document **3**.\\n- So in your answer, you will cite as: (tài liệu tham khảo 1),(tài liệu tham khảo 2).\\n\\n#### 7. Additional Rules:\\n- Do not repeat the question in your answer.\\n- The order of the IDs in the `\"key\"` list must reflect the importance or relevance** of the sources used.The most important sources should come first.\\n- Always cite sources in the format: **(tài liệu tham khảo X)” where **X corresponds to the index number** of the document in the `\"key\"` list (starting from 1).'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyDNFIpO-rqACDC8IE35qs6csgPJKpVWnbk\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "response = model.generate_content(\"Viết bài thơ về quê hương\", stream=True)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_Document_Results, lst_Article_Quote = rag.get_Article_Content_Results(\"Hợp đồng thử việc tối đa nhận lương bao nhiêu ?\")\n",
    "print(article_Document_Results)\n",
    "print(lst_Article_Quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng đoạn tách được: 5\n",
      "Đoạn 1:\n",
      "# Tiêu đề chính\n",
      "---\n",
      "Đoạn 2:\n",
      "## Đoạn 1\n",
      "Nội dung đoạn 1 không có con\n",
      "---\n",
      "Đoạn 3:\n",
      "## Đoạn 2\n",
      "### Con 2.1\n",
      "Nội dung con 2.1\n",
      "### Con 2.2\n",
      "Nội dung con 2.2\n",
      "---\n",
      "Đoạn 4:\n",
      "## Đoạn 3\n",
      "Nội dung đoạn 3 bình thường\n",
      "---\n",
      "Đoạn 5:\n",
      "## Đoạn 4\n",
      "### Con 4.1\n",
      "Chi tiết con 4.1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = '''\n",
    "# Tiêu đề chính\n",
    "\n",
    "## Đoạn 1\n",
    "Nội dung đoạn 1 không có con\n",
    "\n",
    "## Đoạn 2\n",
    "### Con 2.1\n",
    "Nội dung con 2.1\n",
    "### Con 2.2\n",
    "Nội dung con 2.2\n",
    "\n",
    "## Đoạn 3ff\n",
    "Nội dung đoạn 3 bình thường\n",
    "\n",
    "## Đoạn 4\n",
    "### Con 4.1\n",
    "Chi tiết con 4.1\n",
    "'''\n",
    "\n",
    "pattern = r'f'\n",
    "doan = re.split(pattern, text, flags=re.MULTILINE)\n",
    "doan = [d.strip() for d in doan if d.strip()]\n",
    "\n",
    "print(f'Tổng đoạn tách được: {len(doan)}')\n",
    "for i, d in enumerate(doan, 1):\n",
    "    print(f'Đoạn {i}:')\n",
    "    print(d)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Văn bản chứa \"tài liệu tham khảo\"\n",
    "import re\n",
    "\n",
    "def find_references_numbers(text):\n",
    "    # Biểu thức chính quy để bắt tất cả các số sau \"tài liệu tham khảo\"\n",
    "    pattern = r\"tài liệu tham khảo (\\d+)\"\n",
    "    \n",
    "    # Tìm tất cả các số sau \"tài liệu tham khảo\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Trả về danh sách chứa các số tài liệu tham khảo\n",
    "    return matches\n",
    "\n",
    "text = \"\"\"\n",
    "Tài liệu tham khảo 1, Tài liệu tham khảo 2, Tài liệu tham khảo 3, Tài liệu tham khảo 4, Tài liệu tham khảo 5.\n",
    "\"\"\"\n",
    "\n",
    "# Gọi hàm để tìm các số tài liệu tham khảo\n",
    "matches = find_references_numbers(text)\n",
    "\n",
    "# In kết quả (danh sách các số)\n",
    "print(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Duyệt qua kết quả và in chi tiết tài liệu\n",
    "# print(\"\\n--- Kết quả tìm kiếm ---\")\n",
    "# print(len(rerank_documents))\n",
    "# for doc, score in rerank_documents:\n",
    "#     # print(f\"\\nTiêu đề: {doc.metadata.get('title', 'Không có tiêu đề')}\")\n",
    "#     print(f\"Nội dung: {doc}...\")  # In ra 300 ký tự đầu tiên của nội dung tài liệu\n",
    "#     print(f\"Điểm số: {score['doc_metadata']['Loai-Van-Ban']}\")\n",
    "#     # print(f\"Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = PeftConfig.from_pretrained(\"./lora_extract/\")\n",
    "# base_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "#     config.base_model_name_or_path,  # Tên mô hình cơ sở\n",
    "#     torch_dtype=torch.float32,      \n",
    "# )\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import numpy as np\n",
    "# tokenizer=AutoTokenizer.from_pretrained(\"./full_extract\")\n",
    "# config = PeftConfig.from_pretrained(\"./lora_extract\")\n",
    "# base_model = AutoModelForQuestionAnswering.from_pretrained(config.base_model_name_or_path,torch_dtype=torch.float32) \n",
    "# model_extract = PeftModel.from_pretrained(base_model,\"./lora_extract\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./full_extract/\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"./full_extract/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PeftModel.from_pretrained(base_model, \"./lora_extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 177,264,386\n",
      "Trainable parameters: 177,264,386\n",
      "Non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Tính tổng số lượng tham số\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# In ra\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hdang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hdang\\.cache\\huggingface\\hub\\models--google-bert--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\hdang\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "import numpy as np\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "config = PeftConfig.from_pretrained(\"./lora_extract\")\n",
    "base_model = AutoModelForQuestionAnswering.from_pretrained(config.base_model_name_or_path,torch_dtype=torch.float32) \n",
    "model_extract = PeftModel.from_pretrained(base_model,\"./lora_extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,538 || all params: 190,242,052 || trainable%: 0.0008\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_extract.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1871746974.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    trainable_params = sum(p.numel() for p nconfig.parameters() if p.requires_grad)\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Tính tổng số lượng tham số\n",
    "total_params = sum(p.numel() for p in config.parameters())\n",
    "trainable_params = sum(p.numel() for p nconfig.parameters() if p.requires_grad)\n",
    "\n",
    "# In ra\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các layer có tham số được huấn luyện (LoRA):\n",
      "base_model.model.qa_outputs.modules_to_save.default.weight: 1,536 tham số\n",
      "base_model.model.qa_outputs.modules_to_save.default.bias: 2 tham số\n",
      "\n",
      "Tổng số tham số được huấn luyện: 1,538\n"
     ]
    }
   ],
   "source": [
    "# In các layer có LoRA\n",
    "print(\"Các layer có tham số được huấn luyện (LoRA):\")\n",
    "for name, param in model_extract.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel():,} tham số\")\n",
    "\n",
    "# Tổng số trainable\n",
    "trainable_params = sum(p.numel() for p in model_extract.parameters() if p.requires_grad)\n",
    "print(f\"\\nTổng số tham số được huấn luyện: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.qa_outputs.modules_to_save.default.weight: 1,536\n",
      "base_model.model.qa_outputs.modules_to_save.default.bias: 2\n",
      "\n",
      "Total params: 190,242,052\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 190,240,514\n"
     ]
    }
   ],
   "source": [
    "total, trainable = 0, 0\n",
    "for name, param in model_extract.named_parameters():\n",
    "    num = param.numel()\n",
    "    total += num\n",
    "    if param.requires_grad:\n",
    "        trainable += num\n",
    "        print(f\"{name}: {num:,}\")\n",
    "print(f\"\\nTotal params: {total:,}\")\n",
    "print(f\"Trainable params: {trainable:,}\")\n",
    "print(f\"Non-trainable params: {total - trainable:,}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForQuestionAnswering(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertForQuestionAnswering(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=128, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=128, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=128, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=128, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=128, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=128, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (qa_outputs): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "STRIDE = 180\n",
    "N_BEST = 350\n",
    "MAX_ANSWER_LENGTH = 2000\n",
    "def predict(contexts, question):\n",
    "        inputs = tokenizer(\n",
    "            question,\n",
    "            contexts,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=\"only_second\",\n",
    "            stride=STRIDE,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_extract(**{k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']})\n",
    "        \n",
    "        start_logits = outputs.start_logits.squeeze().cpu().numpy()\n",
    "        end_logits = outputs.end_logits.squeeze().cpu().numpy()\n",
    "        offsets = inputs[\"offset_mapping\"][0].cpu().numpy()\n",
    "        \n",
    "        answers = []\n",
    "        start_indexes = np.argsort(start_logits)[-N_BEST:][::-1].tolist()\n",
    "        end_indexes = np.argsort(end_logits)[-N_BEST:][::-1].tolist()\n",
    "\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                if end_index < start_index or end_index - start_index + 1 > MAX_ANSWER_LENGTH:\n",
    "                    continue\n",
    "                if offsets[start_index][0] is not None and offsets[end_index][1] is not None:\n",
    "                    answer_text = contexts[offsets[start_index][0]: offsets[end_index][1]].strip()\n",
    "                    if answer_text:\n",
    "                        answer = {\n",
    "                            \"text\": answer_text,\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                        }\n",
    "                        answers.append(answer)\n",
    "        \n",
    "        if answers:\n",
    "            answers.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            best_answer = answers[0]['text']\n",
    "            return best_answer\n",
    "        else: \n",
    "            return \"Không có câu trả lời\"\n",
    "def remove_underscore(text):\n",
    "    return text.replace(\"_\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời_gian thử việc đối_với từng loại công_việc không được vượt quá : \n",
      " \n",
      " a ) 180 ngày đối_với công_việc có chức_danh quản_lý ; \n",
      " \n",
      " b ) 60 ngày đối_với công_việc cần trình_độ chuyên_môn , kỹ_thuật từ cao_đẳng trở lên ; \n",
      " \n",
      " c ) 30 ngày đối_với công_việc cần trình_độ trung_cấp , công_nhân kỹ_thuật , nhân_viên nghiệp_vụ ; \n",
      " \n",
      " d ) 6 ngày làm_việc đối_với các công_việc khác .\n"
     ]
    }
   ],
   "source": [
    "from pyvi import ViTokenizer\n",
    "paragraps=\"\"\" Điều 27. Nghĩa vụ và quyền lợi của người lao động trong thời gian thử việc\n",
    "Người lao động và người sử dụng lao động có thể thỏa thuận về việc làm thử, thời gian thử việc và quyền, nghĩa vụ của các bên trong thời gian thử việc.\n",
    "\n",
    "Thời gian thử việc đối với từng loại công việc không được vượt quá:\n",
    "\n",
    "a) 180 ngày đối với công việc có chức danh quản lý;\n",
    "\n",
    "b) 60 ngày đối với công việc cần trình độ chuyên môn, kỹ thuật từ cao đẳng trở lên;\n",
    "\n",
    "c) 30 ngày đối với công việc cần trình độ trung cấp, công nhân kỹ thuật, nhân viên nghiệp vụ;\n",
    "\n",
    "d) 6 ngày làm việc đối với các công việc khác.\n",
    "\n",
    "Trong thời gian thử việc, người lao động được hưởng mức lương do hai bên thỏa thuận nhưng không được thấp hơn 85% mức lương chính thức của vị trí đó.\n",
    "\n",
    "Người lao động trong thời gian thử việc có quyền đơn phương chấm dứt hợp đồng thử việc mà không cần báo trước và không phải bồi thường. Người sử dụng lao động cũng có quyền chấm dứt hợp đồng thử việc nếu xét thấy người lao động không đạt yêu cầu công việc.\n",
    "\n",
    "Khi kết thúc thời gian thử việc, nếu người lao động đạt yêu cầu thì người sử dụng lao động phải ký hợp đồng lao động chính thức. Trường hợp không ký hợp đồng lao động thì phải thông báo bằng văn bản nêu rõ lý do.\n",
    "\"\"\"\n",
    "\n",
    "question=\"thời gian thử việc không vượt quá bao nhiêu ngày ?\"\n",
    "result=predict(ViTokenizer.tokenize(paragraps),ViTokenizer.tokenize(question))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def clean_code_fence_safe(text: str) -> str:\n",
    "    lines = text.strip().splitlines()\n",
    "    if lines and lines[0].strip().startswith(\"```\"):\n",
    "        # Nếu dòng đầu chỉ chứa dấu ``` hoặc ```json\n",
    "        if lines[0].strip() == \"```\" or lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "    if lines and lines[-1].strip() == \"```\":\n",
    "        lines = lines[:-1]\n",
    "    return \"\\n\".join(lines).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"```json\n",
    "{\n",
    "  \"answer\": \"Chào bạn, theo quy định của pháp luật Việt Nam, trẻ em khuyết tật, mắc bệnh hiểm nghèo vẫn có thể được nhận làm con nuôi trong một số trường hợp nhất định. Dưới đây là các trường hợp cụ thể: \\n\\n*   **Trường hợp con nuôi là đối tượng đặc biệt:**\\n    *   Nếu trẻ em bị khuyết tật, nhiễm HIV/AIDS hoặc mắc bệnh hiểm nghèo khác.\\n*   **Trường hợp người nhận con nuôi là người thân thích hoặc người nước ngoài làm việc, học tập tại Việt Nam:**\\n    *   Cô, cậu, dì, chú, bác ruột của người được nhận làm con nuôi.\\n    *   Người nước ngoài đang làm việc, học tập ở Việt Nam trong thời gian ít nhất là 01 năm.\\n\\nNhư vậy, trẻ em khuyết tật, mắc bệnh hiểm nghèo vẫn có thể được nhận làm con nuôi nếu thuộc các trường hợp trên.\",\n",
    "  \"key\": [\n",
    "    1\n",
    "  ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=clean_code_fence_safe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"answer\": \"Chào bạn, theo quy định của pháp luật Việt Nam, trẻ em khuyết tật, mắc bệnh hiểm nghèo vẫn có thể được nhận làm con nuôi trong một số trường hợp nhất định. Dưới đây là các trường hợp cụ thể: \n",
      "\n",
      "*   **Trường hợp con nuôi là đối tượng đặc biệt:**\n",
      "    *   Nếu trẻ em bị khuyết tật, nhiễm HIV/AIDS hoặc mắc bệnh hiểm nghèo khác.\n",
      "*   **Trường hợp người nhận con nuôi là người thân thích hoặc người nước ngoài làm việc, học tập tại Việt Nam:**\n",
      "    *   Cô, cậu, dì, chú, bác ruột của người được nhận làm con nuôi.\n",
      "    *   Người nước ngoài đang làm việc, học tập ở Việt Nam trong thời gian ít nhất là 01 năm.\n",
      "\n",
      "Như vậy, trẻ em khuyết tật, mắc bệnh hiểm nghèo vẫn có thể được nhận làm con nuôi nếu thuộc các trường hợp trên.\",\n",
      "  \"key\": [\n",
      "    1\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json_string(s):\n",
    "    # Bước 1: Thay xuống dòng thật trong chuỗi thành \\n chỉ trong giá trị của key \"answer\"\n",
    "    # Dùng regex để lấy phần trong dấu \"\"\n",
    "    match = re.search(r'\"answer\"\\s*:\\s*\"(.+?)\"\\s*,', s, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return s  # Không tìm thấy \"answer\", trả về nguyên bản\n",
    "\n",
    "    answer_str = match.group(1)\n",
    "    # Thay xuống dòng thật thành \\n\n",
    "    answer_fixed = answer_str.replace('\\n', '\\\\n')\n",
    "\n",
    "    # Thay lại trong chuỗi gốc\n",
    "    s_fixed = s.replace(answer_str, answer_fixed)\n",
    "    return s_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "fixed_result = fix_json_string(result)\n",
    "data = json.loads(fixed_result)\n",
    "print(data['key'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
